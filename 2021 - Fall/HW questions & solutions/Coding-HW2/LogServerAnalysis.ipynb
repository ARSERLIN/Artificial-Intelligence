{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"colab":{"name":"ML1-LogServerAnalysis.ipynb","provenance":[],"collapsed_sections":["devoted-diabetes","concerned-compound","collectible-makeup","educational-accuracy","coastal-virginia","cooperative-matrix","worldwide-removal","faced-willow","satellite-murder"],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"final-national"},"source":["# Log Server Analysis\n","\n","This is the assignment **#1** related to **Machine Learning** section of the **Artificial Intelligence** course at **Shahid Beheshti University**. ([Course repository link](https://github.com/SBU-CE/Artificial-Intelligence)).\n","\n","In this notebook, we will take advantage of three principal python libraries for data science and machine learning tasks.\n","\n"," - [Numpy](https://numpy.org): The fundamental package for **scientific computing** with Python.\n"," - [Pandas](https://pandas.pydata.org): An open source **data analysis and manipulation tool**, built on top of the Python programming language.\n"," - [Matplotlib](https://matplotlib.org) : A comprehensive library for **creating static, animated, and interactive visualizations** in Python.\n"," \n"," \n"," **Before you start:** Please read the ***Submission*** section at the bottom of the notebook carefully."],"id":"final-national"},{"cell_type":"markdown","metadata":{"id":"american-desperate"},"source":["# 0. What Is Log Analysis & Why Is It Important? "],"id":"american-desperate"},{"cell_type":"markdown","metadata":{"id":"cardiac-student"},"source":["**Log analysis** is the process of making sense of computer-generated log messages, also known as log events, audit trail records, or simply logs. Most businesses are required to archive and analyze logs of their websites requests from their clients as part of their compliance regulations. \n","\n","They must regularly perform system log monitoring and analysis to search for **errors**, **anomalies**, or **suspicious** or **unauthorized activity** that deviates from the norm. Log analysis allows them to re-create the chain of events that led up to a problem and effectively troubleshoot it.\n","\n","Web sever logs contain information on any event that was registered/logged. This contains a lot of insights on website visitors, behavior, crawlers accessing the site, business insights, security issues, and more."],"id":"cardiac-student"},{"cell_type":"markdown","metadata":{"id":"afraid-transsexual"},"source":["## Data\n","\n","In this notebook, we will use the 3.3GB of logs from an Iranian ecommerce website [zanbil.ir](zanbil.ir). \n","\n","**Acknowledgements**: \n","Zaker, Farzin, 2019, \"*Online Shopping Store - Web Server Logs*\", https://doi.org/10.7910/DVN/3QBYB5, Harvard Dataverse, V1.\n","\n","Please download the raw dataset `access.log` from https://www.kaggle.com/eliasdabbas/web-server-access-logs([~ 264MB]) and put it besides this notebook in the current directory.\n","\n","So let's dive into it!"],"id":"afraid-transsexual"},{"cell_type":"markdown","metadata":{"id":"residential-mainstream"},"source":["## Setup\n","\n","We will heavily use NumPy, Pandas and Matplotlib libraries in this assignment. Notice that you are free to import other libraries in case they are needed."],"id":"residential-mainstream"},{"cell_type":"code","metadata":{"id":"recovered-habitat"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"],"id":"recovered-habitat","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"municipal-elevation"},"source":["# 1. Convert a Log File Into an Efficient DataFrame\n","\n","This first step is the prototype of a process of converting a log file to an efficient format on disk (Apache Parquet), and then to read it into an efficient DataFrame with optimized datatypes.\n","\n","In this part we convert a 3.3 GB text file to a 256 MB parquet file, which is later read into a 297 MB DataFrame. The total time can vary between two to five minutes, depending on the system used. \n","\n","**Note**: Make sure to **sequentially run all the cells**, so that the intermediate variables / packages will carry over to the next cell."],"id":"municipal-elevation"},{"cell_type":"code","metadata":{"id":"electric-classic","outputId":"c32115b2-2c8a-415d-a6ac-6fc49255fcd4"},"source":["# read some sample lines from the log file\n","!head -n 3 access.log"],"id":"electric-classic","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["54.36.149.41 - - [22/Jan/2019:03:56:14 +0330] \"GET /filter/27|13%20%D9%85%DA%AF%D8%A7%D9%BE%DB%8C%DA%A9%D8%B3%D9%84,27|%DA%A9%D9%85%D8%AA%D8%B1%20%D8%A7%D8%B2%205%20%D9%85%DA%AF%D8%A7%D9%BE%DB%8C%DA%A9%D8%B3%D9%84,p53 HTTP/1.1\" 200 30577 \"-\" \"Mozilla/5.0 (compatible; AhrefsBot/6.1; +http://ahrefs.com/robot/)\" \"-\"\r\n","31.56.96.51 - - [22/Jan/2019:03:56:16 +0330] \"GET /image/60844/productModel/200x200 HTTP/1.1\" 200 5667 \"https://www.zanbil.ir/m/filter/b113\" \"Mozilla/5.0 (Linux; Android 6.0; ALE-L21 Build/HuaweiALE-L21) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.158 Mobile Safari/537.36\" \"-\"\r\n","31.56.96.51 - - [22/Jan/2019:03:56:16 +0330] \"GET /image/61474/productModel/200x200 HTTP/1.1\" 200 5379 \"https://www.zanbil.ir/m/filter/b113\" \"Mozilla/5.0 (Linux; Android 6.0; ALE-L21 Build/HuaweiALE-L21) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.158 Mobile Safari/537.36\" \"-\"\r\n"]}]},{"cell_type":"markdown","metadata":{"id":"solid-fancy"},"source":["## Log Format\n","\n","This approach assumes the common log format and/or the combined one, which are two of the most commonly used. Eventually other formats can be incorporated. We start with the below regular expression taken from:\n","\n","[Regular Expression Cookbook](https://www.oreilly.com/library/view/regular-expressions-cookbook/9781449327453/ch07s12.html)\n","\n","by *Jan Goyvaerts, Steven Levithan*\n","\n","Publisher: O'Reilly Media, Inc. Release Date: August 2012"],"id":"solid-fancy"},{"cell_type":"code","metadata":{"id":"forward-zimbabwe"},"source":["# There is a minor bug in this regex, it misses the last field. we'll fix this soon. \n","\n","common_regex = '^(?P<client>\\S+) \\S+ (?P<userid>\\S+) \\[(?P<datetime>[^\\]]+)\\] \"(?P<method>[A-Z]+) (?P<request>[^ \"]+)? HTTP/[0-9.]+\" (?P<status>[0-9]{3}) (?P<size>[0-9]+|-)'\n","combined_regex = '^(?P<client>\\S+) \\S+ (?P<userid>\\S+) \\[(?P<datetime>[^\\]]+)\\] \"(?P<method>[A-Z]+) (?P<request>[^ \"]+)? HTTP/[0-9.]+\" (?P<status>[0-9]{3}) (?P<size>[0-9]+|-) \"(?P<referrer>[^\"]*)\" \"(?P<useragent>[^\"]*)'\n","columns = ['client', 'userid', 'datetime', 'method', 'request', 'status', 'size', 'referer', 'user_agent']"],"id":"forward-zimbabwe","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"biological-skating"},"source":["## The Approach\n","\n","* Loop through the lines of the input log file one by one. This ensures minimal memory consumption. \n","* For each line, check it against the regular expression, and process it: \n","  * Match: append the matched line to a `parsed_lines` list\n","  * No match: append the non-matching line to the `errors_file` for later analysis\n","* Once `parsed_lines` reaches 250,000 elements, convert the list to a DataFrame and save it to a `parquet` file in the `output_dir`. Clear the list. This also ensures minimal memory usage, and the 250k can be tweaked if necessary.\n","* Read all the files of the `output_dir` with `read_parquet` into a pandas DataFrame. This function handles reading all the files and combines them. \n","* Optimize the columns by using more efficient data types, most notably the pandas categorical type.\n","* Write the DataFrame to a single file, for more convenient handling, and with the more efficient datatypes. This results in even faster reading.\n","* Delete the files in `output_dir`.\n","* Read in the final file with `read_parquet`.\n","* Happy log server analyzing!\n"],"id":"biological-skating"},{"cell_type":"code","metadata":{"id":"moving-bankruptcy"},"source":["# Create a destinatoin directory where output files will be stored\n","%mkdir parquet_dir"],"id":"moving-bankruptcy","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"animal-cloud"},"source":["# install some usefull packages for converting log file to parquet format\n","\n","%pip install pyarrow\n","%pip install fastparquet"],"id":"animal-cloud","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"foreign-nursing"},"source":["## The `logs_to_df` Function\n","\n","This function will convert the `access.log` file into an optimized `DataFrame` and also write the errors happend during converting (e.g. some lines of the log file are not following the common format of nginx log) intp `errors.txt`."],"id":"foreign-nursing"},{"cell_type":"code","metadata":{"id":"adult-farmer"},"source":["import time\n","import re\n","from tqdm import tqdm\n","\n","def logs_to_df(logfile, output_dir, errors_file):\n","    with open(logfile) as source_file:\n","        linenumber = 0\n","        parsed_lines = []\n","        for line in tqdm(source_file):\n","            try:\n","                log_line = re.findall(combined_regex, line)[0]\n","                parsed_lines.append(log_line)\n","            except Exception as e:\n","                with open(errors_file, 'at') as errfile:\n","                    print((line, str(e)), file=errfile)\n","                continue\n","            linenumber += 1\n","            if linenumber % 250_000 == 0:\n","                df = pd.DataFrame(parsed_lines, columns=columns)\n","                df.to_parquet(f'{output_dir}/file_{linenumber}.parquet')\n","                parsed_lines.clear()\n","        else:\n","            df = pd.DataFrame(parsed_lines, columns=columns)\n","            df.to_parquet(f'{output_dir}/file_{linenumber}.parquet')\n","            parsed_lines.clear()"],"id":"adult-farmer","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"increasing-cradle","outputId":"8a08cbb3-1c4b-4203-d4c0-c687863ae31a"},"source":["# Times will vary from system to system, and I will use the approximate values, \n","# so when you read this, you will likely see slightly different numbers.\n","\n","%time logs_to_df(logfile='access.log', output_dir='parquet_dir/', errors_file='errors.txt')"],"id":"increasing-cradle","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["10365152it [01:01, 168324.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 58.3 s, sys: 2.8 s, total: 1min 1s\n","Wall time: 1min 2s\n"]}]},{"cell_type":"markdown","metadata":{"id":"seven-architect"},"source":["Actually we are now ready to start analysis, as we have the parquet files that can be read. But we will optimize them even more.\n","\n","Just let's check the number of resulting parsing errors:"],"id":"seven-architect"},{"cell_type":"code","metadata":{"id":"married-estate","outputId":"7e149881-960c-4322-fed1-93edb8a63e70"},"source":["!wc errors.txt"],"id":"married-estate","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["     287    7037  228081 errors.txt\r\n"]}]},{"cell_type":"markdown","metadata":{"id":"lyric-latest"},"source":["Now we are able to read the whole parquet files. Reading the whole directory takes about nine seconds. We now check the size of the resulting directory on disk:"],"id":"lyric-latest"},{"cell_type":"code","metadata":{"id":"tutorial-promise","outputId":"24e0425d-ddef-4ff0-d53f-b3d4e58761c6"},"source":["%time logs_df = pd.read_parquet('parquet_dir/')"],"id":"tutorial-promise","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 13.2 s, sys: 12.7 s, total: 25.9 s\n","Wall time: 18.2 s\n"]}]},{"cell_type":"code","metadata":{"id":"activated-strip","outputId":"f0a3b4d4-5534-4d63-bc88-7f1f8cec0dba"},"source":["!du -sh parquet_dir/"],"id":"activated-strip","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["256M\tparquet_dir/\r\n"]}]},{"cell_type":"markdown","metadata":{"id":"cosmetic-fitting"},"source":["256 ÷ 3,300 = 0.07.\n","\n","The resulting file is **7%** the size of the original!\n","\n","Let's see how much memory it takes:"],"id":"cosmetic-fitting"},{"cell_type":"code","metadata":{"id":"portable-squad","outputId":"e34a1251-d0ef-413f-bd85-317018ce6182"},"source":["logs_df.info(show_counts=True, verbose=True)"],"id":"portable-squad","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10364865 entries, 0 to 10364864\n","Data columns (total 9 columns):\n"," #   Column      Non-Null Count     Dtype \n","---  ------      --------------     ----- \n"," 0   client      10364865 non-null  object\n"," 1   userid      10364865 non-null  object\n"," 2   datetime    10364865 non-null  object\n"," 3   method      10364865 non-null  object\n"," 4   request     10364865 non-null  object\n"," 5   status      10364865 non-null  object\n"," 6   size        10364865 non-null  object\n"," 7   referer     10364865 non-null  object\n"," 8   user_agent  10364865 non-null  object\n","dtypes: object(9)\n","memory usage: 711.7+ MB\n"]}]},{"cell_type":"markdown","metadata":{"id":"special-planet"},"source":["711 MB. We now can remove the files in `parquet_dir` and optimize the datatypes and use more efficient ones. \n","\n","**Note**: You can keep it for later usages. Since the kernel will die after you terminate the notebook, the dataframe will be removed from memory."],"id":"special-planet"},{"cell_type":"code","metadata":{"id":"guilty-midwest"},"source":["# %rm -r parquet_dir/"],"id":"guilty-midwest","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"incredible-choir"},"source":["Now we optimize the dataframe by changing the column data types. It may take a few seconds:"],"id":"incredible-choir"},{"cell_type":"code","metadata":{"id":"exceptional-elimination"},"source":["logs_df['client'] = logs_df['client'].astype('category')\n","del logs_df['userid']\n","logs_df['datetime'] = pd.to_datetime(logs_df['datetime'], format='%d/%b/%Y:%H:%M:%S %z')\n","logs_df['method'] = logs_df['method'].astype('category')\n","logs_df['status'] = logs_df['status'].astype('int16')\n","logs_df['size'] = logs_df['size'].astype('int32')\n","logs_df['referer'] = logs_df['referer'].astype('category')\n","logs_df['user_agent'] = logs_df['user_agent'].astype('category')"],"id":"exceptional-elimination","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"medieval-possession"},"source":["**Note**: Some types are considered as `Categorical`. (client, method, referer and user_agent). You may need to convert them to *string* in some questions."],"id":"medieval-possession"},{"cell_type":"code","metadata":{"id":"critical-cotton","outputId":"cb5a4500-2e5b-4d86-84bc-1bbfde1c22f1"},"source":["logs_df.info(verbose=True, show_counts=True)"],"id":"critical-cotton","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10364865 entries, 0 to 10364864\n","Data columns (total 8 columns):\n"," #   Column      Non-Null Count     Dtype                                \n","---  ------      --------------     -----                                \n"," 0   client      10364865 non-null  category                             \n"," 1   datetime    10364865 non-null  datetime64[ns, pytz.FixedOffset(210)]\n"," 2   method      10364865 non-null  category                             \n"," 3   request     10364865 non-null  object                               \n"," 4   status      10364865 non-null  int16                                \n"," 5   size        10364865 non-null  int32                                \n"," 6   referer     10364865 non-null  category                             \n"," 7   user_agent  10364865 non-null  category                             \n","dtypes: category(4), datetime64[ns, pytz.FixedOffset(210)](1), int16(1), int32(1), object(1)\n","memory usage: 342.3+ MB\n"]}]},{"cell_type":"markdown","metadata":{"id":"clean-crown"},"source":["The file was reduced further from 711 to 298 MB. (342 ÷ 711 = 0.42 of the original size).\n","\n","We now save it to a single file, and read again."],"id":"clean-crown"},{"cell_type":"code","metadata":{"id":"gothic-portable"},"source":["logs_df.to_parquet('logs_df.parquet')"],"id":"gothic-portable","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"devoted-diabetes"},"source":["## Read the Parquet file into a DataFrame"],"id":"devoted-diabetes"},{"cell_type":"code","metadata":{"id":"adult-rugby","outputId":"8ca11789-1764-4a22-c2d0-24b97e7db078"},"source":["# Read the optimized parquet file into a dataframe\n","logs_df = pd.read_parquet('logs_df.parquet')\n","\n","print ('DataFrame shape: {}'.format(logs_df.shape))\n","logs_df.head(10)"],"id":"adult-rugby","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["DataFrame shape: (10364865, 8)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>client</th>\n","      <th>datetime</th>\n","      <th>method</th>\n","      <th>request</th>\n","      <th>status</th>\n","      <th>size</th>\n","      <th>referer</th>\n","      <th>user_agent</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37.152.163.59</td>\n","      <td>2019-01-22 12:38:27+03:30</td>\n","      <td>GET</td>\n","      <td>/image/29314?name=%D8%AF%DB%8C%D8%A8%D8%A7-7.j...</td>\n","      <td>200</td>\n","      <td>1105</td>\n","      <td>https://www.zanbil.ir/product/29314/%DA%A9%D8%...</td>\n","      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7....</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>37.152.163.59</td>\n","      <td>2019-01-22 12:38:27+03:30</td>\n","      <td>GET</td>\n","      <td>/static/images/zanbil-kharid.png</td>\n","      <td>200</td>\n","      <td>358</td>\n","      <td>https://www.zanbil.ir/product/29314/%DA%A9%D8%...</td>\n","      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7....</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>85.9.73.119</td>\n","      <td>2019-01-22 12:38:27+03:30</td>\n","      <td>GET</td>\n","      <td>/static/images/next.png</td>\n","      <td>200</td>\n","      <td>3045</td>\n","      <td>https://znbl.ir/static/bundle-bundle_site_head...</td>\n","      <td>Mozilla/5.0 (Windows NT 6.1; Win64; x64) Apple...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>37.152.163.59</td>\n","      <td>2019-01-22 12:38:27+03:30</td>\n","      <td>GET</td>\n","      <td>/image/29314?name=%D8%AF%DB%8C%D8%A8%D8%A7-4.j...</td>\n","      <td>200</td>\n","      <td>1457</td>\n","      <td>https://www.zanbil.ir/product/29314/%DA%A9%D8%...</td>\n","      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7....</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>85.9.73.119</td>\n","      <td>2019-01-22 12:38:27+03:30</td>\n","      <td>GET</td>\n","      <td>/static/images/checked.png</td>\n","      <td>200</td>\n","      <td>1083</td>\n","      <td>https://znbl.ir/static/bundle-bundle_site_head...</td>\n","      <td>Mozilla/5.0 (Windows NT 6.1; Win64; x64) Apple...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>37.152.163.59</td>\n","      <td>2019-01-22 12:38:27+03:30</td>\n","      <td>GET</td>\n","      <td>/static/images/loading.gif</td>\n","      <td>200</td>\n","      <td>7370</td>\n","      <td>https://www.zanbil.ir/product/29314/%DA%A9%D8%...</td>\n","      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7....</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>77.245.233.52</td>\n","      <td>2019-01-22 12:38:27+03:30</td>\n","      <td>GET</td>\n","      <td>/image/11082/productType/240x180</td>\n","      <td>200</td>\n","      <td>12458</td>\n","      <td>https://www.zanbil.ir/browse/sports/%D8%AA%D8%...</td>\n","      <td>Mozilla/5.0 (Windows NT 6.1; rv:64.0) Gecko/20...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>37.27.128.139</td>\n","      <td>2019-01-22 12:38:27+03:30</td>\n","      <td>GET</td>\n","      <td>/browse/Tablet-Arm-Chair/%D8%B5%D9%86%D8%AF%D9...</td>\n","      <td>200</td>\n","      <td>30604</td>\n","      <td>https://www.zanbil.ir/browse/Classroom-Furnitu...</td>\n","      <td>Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.3...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>77.245.233.52</td>\n","      <td>2019-01-22 12:38:27+03:30</td>\n","      <td>GET</td>\n","      <td>/image/851/mainSlide</td>\n","      <td>200</td>\n","      <td>89859</td>\n","      <td>https://www.zanbil.ir/browse/sports/%D8%AA%D8%...</td>\n","      <td>Mozilla/5.0 (Windows NT 6.1; rv:64.0) Gecko/20...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>77.245.233.52</td>\n","      <td>2019-01-22 12:38:27+03:30</td>\n","      <td>GET</td>\n","      <td>/image/848/mainSlide</td>\n","      <td>200</td>\n","      <td>93168</td>\n","      <td>https://www.zanbil.ir/browse/sports/%D8%AA%D8%...</td>\n","      <td>Mozilla/5.0 (Windows NT 6.1; rv:64.0) Gecko/20...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          client                  datetime method  \\\n","0  37.152.163.59 2019-01-22 12:38:27+03:30    GET   \n","1  37.152.163.59 2019-01-22 12:38:27+03:30    GET   \n","2    85.9.73.119 2019-01-22 12:38:27+03:30    GET   \n","3  37.152.163.59 2019-01-22 12:38:27+03:30    GET   \n","4    85.9.73.119 2019-01-22 12:38:27+03:30    GET   \n","5  37.152.163.59 2019-01-22 12:38:27+03:30    GET   \n","6  77.245.233.52 2019-01-22 12:38:27+03:30    GET   \n","7  37.27.128.139 2019-01-22 12:38:27+03:30    GET   \n","8  77.245.233.52 2019-01-22 12:38:27+03:30    GET   \n","9  77.245.233.52 2019-01-22 12:38:27+03:30    GET   \n","\n","                                             request  status   size  \\\n","0  /image/29314?name=%D8%AF%DB%8C%D8%A8%D8%A7-7.j...     200   1105   \n","1                   /static/images/zanbil-kharid.png     200    358   \n","2                            /static/images/next.png     200   3045   \n","3  /image/29314?name=%D8%AF%DB%8C%D8%A8%D8%A7-4.j...     200   1457   \n","4                         /static/images/checked.png     200   1083   \n","5                         /static/images/loading.gif     200   7370   \n","6                   /image/11082/productType/240x180     200  12458   \n","7  /browse/Tablet-Arm-Chair/%D8%B5%D9%86%D8%AF%D9...     200  30604   \n","8                               /image/851/mainSlide     200  89859   \n","9                               /image/848/mainSlide     200  93168   \n","\n","                                             referer  \\\n","0  https://www.zanbil.ir/product/29314/%DA%A9%D8%...   \n","1  https://www.zanbil.ir/product/29314/%DA%A9%D8%...   \n","2  https://znbl.ir/static/bundle-bundle_site_head...   \n","3  https://www.zanbil.ir/product/29314/%DA%A9%D8%...   \n","4  https://znbl.ir/static/bundle-bundle_site_head...   \n","5  https://www.zanbil.ir/product/29314/%DA%A9%D8%...   \n","6  https://www.zanbil.ir/browse/sports/%D8%AA%D8%...   \n","7  https://www.zanbil.ir/browse/Classroom-Furnitu...   \n","8  https://www.zanbil.ir/browse/sports/%D8%AA%D8%...   \n","9  https://www.zanbil.ir/browse/sports/%D8%AA%D8%...   \n","\n","                                          user_agent  \n","0  Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7....  \n","1  Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7....  \n","2  Mozilla/5.0 (Windows NT 6.1; Win64; x64) Apple...  \n","3  Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7....  \n","4  Mozilla/5.0 (Windows NT 6.1; Win64; x64) Apple...  \n","5  Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7....  \n","6  Mozilla/5.0 (Windows NT 6.1; rv:64.0) Gecko/20...  \n","7  Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.3...  \n","8  Mozilla/5.0 (Windows NT 6.1; rv:64.0) Gecko/20...  \n","9  Mozilla/5.0 (Windows NT 6.1; rv:64.0) Gecko/20...  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"shared-referral"},"source":["# 2. Exploratory Data Analysis (EDA)\n","\n","What's EDA? **Exploratory Data Analysis (EDA)** is an approach for data analysis that employs a variety of techniques (mostly graphical) to\n","\n","  - Maximize insight into a data set.\n","  - Uncover underlying structure.\n","  - Extract important variables.\n","  - Detect outliers and anomalies.\n","  - Test underlying assumptions.\n","  \n","EDA is a primary task before each data-driven algorithms. Most EDA techniques are **graphical** in nature with a few quantitative techniques. The reason for the heavy reliance on graphics is that by its very nature the main role of EDA is to open-mindedly explore, and graphics gives the analysts unparalleled power to do so, enticing the data to reveal its structural secrets, and being always ready to gain some new, often unsuspected, insight into the data.\n","\n","In this assignment, you will implement some of these methods step by step."],"id":"shared-referral"},{"cell_type":"markdown","metadata":{"id":"concerned-compound"},"source":["## Q1. Extract the most visited clients. (10 points)\n","\n","What are the top n'th visited clients? Fill the function below to: \n","\n","   1. Return the top n'th visited IPs as `pandas.DataFrame`\n","   2. Plot them within a **horizontal bar chart**. You can use the matplotlib [documentation](https://matplotlib.org/stable/gallery/lines_bars_and_markers/barh.html)."],"id":"concerned-compound"},{"cell_type":"code","metadata":{"id":"quick-officer"},"source":["def top_clients(df, n=10):\n","    # TODO: Implement this function that takes a pd.DataFrame and n\n","    # as an integer number and returns the top n'th visited IPs(clients).\n","    # Also it must plot a horizontal bar chart with appropriate labels and colors.\n","    # Note that your code must work for all n's as input.\n","    \n","    top_n_visited_clients = pd.DataFrame()\n","    \n","    ############# Your code here ############\n","    \n","    #########################################\n","    \n","    return top_n_visited_clients\n","\n","\n","top_n_visited_clients = top_clients(logs_df, n=10)\n","assert type(top_n_visited_clients) == pd.DataFrame"],"id":"quick-officer","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"collectible-makeup"},"source":["## Q2. What are the most frequent endpoints? (10 points)\n","\n","The `request` column in the main DataFrame is indicating the endpoints which clients has requested. Fill the function below to:\n","\n","  1. Return the top n'th frequent endpoints as a `pandas.DataFrame`\n","  2. Plot them within a **Pie chart**. You can use the matplotlib [documentation](https://matplotlib.org/stable/gallery/pie_and_polar_charts/pie_features.html) as well."],"id":"collectible-makeup"},{"cell_type":"code","metadata":{"id":"quantitative-local"},"source":["def top_endpoints(logs_df, n=5):\n","    # TODO: Implement this function that takes a pd.DataFrame and n\n","    # as an integer number and returns the top n'th frequent endpoints.\n","    # Also it must plot a pie bar chart with appropriate labels and colors.\n","    \n","    # Note that your code must work for all n's as input.\n","    \n","    top_n_freq_endpoints = pd.DataFrame()\n","    \n","    ############# Your code here ############\n","    \n","    #########################################\n","    \n","    return top_n_freq_endpoints\n","\n","\n","top_n_freq_endpoints = top_endpoints(logs_df, n=5)\n","assert type(top_n_freq_endpoints) == pd.DataFrame"],"id":"quantitative-local","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"educational-accuracy"},"source":["## Q3. More in Endpoints! (10 points)\n","\n","In this question you **must** use **regex** to do some processing on endpoints (`request` column).\n","\n"," 1. Remove all the query parameters in the endpoints. e.g. `/image/29314?name=%D8%AF%DB%8C%D8%A8%D8%A7-4.js` must be transformed to `/image/29314.js`\n"," \n"," 2. Then, what porportion of the endpoints including `/image` in their address, are really images? You can assume that images must only have `jpg`, `png`, `jpeg`, and `webp` extensions."],"id":"educational-accuracy"},{"cell_type":"code","metadata":{"id":"reasonable-evaluation"},"source":["def image_endpoints(logs_df):\n","    # TODO: Implement this function that takes a pd.DataFrame\n","    # and returns the (i) a preprocessed logs_df DataFrame.\n","    # Also (ii) a float number between [0, 1] which indicates the porportion of endpoints including \n","    # /image in their address being image files.\n","\n","    # Note: For the first part have to use python regex and avoid any loops.\n","    \n","    real_img_percentage = 0.0\n","    \n","    ############# Your code here ############\n","    \n","    #########################################\n","    \n","    return logs_df, real_img_percentage\n","\n","preprocessed_df, real_img_percentage = image_endpoints(logs_df)\n","assert preprocessed_df.shape == logs_df.shape \n","assert  real_img_percentage <= 1 and real_img_percentage >= 0"],"id":"reasonable-evaluation","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"coastal-virginia"},"source":["## Q4. What can we infer from the status codes? (15 points)\n","\n","As we know, each request get a response code from the server which has it own meanings. Here is a summary for status code families definition:\n","\n"," - **1xx**: An informational response indicates that the request was received and understood.\n"," - **2xx**: This class of status codes indicates the action requested by the client was received, understood, and accepted.\n"," - **3xx**: This class of status code indicates the client must take additional action to complete the request. Many of these status codes are used in URL redirection.\n"," - **4xx**: This class of status code is intended for situations in which the error seems to have been caused by the client\n"," - **5xx**: The server failed to fulfil a request.\n"," \n"," Read more about individual status codes [here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes).\n"," \n"," \n","In this part, we want the **distribution** of each status codes has occured in the log file. Additionally we  want to notice that on which **hours** during a day the server may response to the client using error messages.\n","\n","**Note**: In this question you may need to use `pandas.to_datetime`."],"id":"coastal-virginia"},{"cell_type":"code","metadata":{"id":"discrete-affiliation"},"source":["def status_codes_ditribution(logs_df):\n","    # TODO: Implement this function that takes a pd.DataFrame\n","    # and (i) return a python list of unique values of the status codes\n","    # and (ii) plot a vertical bar chart of status codes frequencies.\n","    \n","    status_codes_list = None\n","    \n","    ############# Your code here ############\n","    \n","    #########################################\n","    \n","    return status_codes_list\n","\n","status_codes_list = status_codes_ditribution(logs_df)"],"id":"discrete-affiliation","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"polish-toddler"},"source":["We want to create a DataFrame containing the total number of **4xx** and **5xx** responses for requests for each hour of the day. You must create a **2-column** DataFrame for each 4xx and 5xx families. This DataFrame must include **24 rows** which are corresponded to hours of a day.\n","\n","The structure of desired DataFrame is as followes:\n","\n","| hour | 4xx | 5xx | \n","| --- | --- | --- |\n","| 0 | ... | ... |\n","| 1 | ... | ... |\n","| 2 | ... | ... |\n","| ... | ... | ... |\n","| 21 | ... | ... |\n","| 22 | ... | ... |\n","| 23| ... | ... |\n"],"id":"polish-toddler"},{"cell_type":"code","metadata":{"id":"sharing-spending"},"source":["def hourly_errors(logs_df):\n","    # TODO: Implement this function that takes a pd.DataFrame\n","    # and returns a 8x2 DataFrame described above (Duration is index)\n","    \n","    # Note: Remember that in this assignment(even in your Data-Science life!) you are not\n","    # allowed to use loops iterating over a DataFrame.\n","    \n","    errors_freq_df = pd.DataFrame()\n","    \n","    ############# Your code here ############\n","    \n","    #########################################\n","    \n","    return errors_freq_df\n","\n","errors_freq_df = hourly_errors(logs_df)\n","assert errors_freq_df.shape == (24, 2)"],"id":"sharing-spending","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cooperative-matrix"},"source":["## Q5. How can user agents help us to recognize bots readily? (10 points)\n","\n","What is user agent? Please read [this](https://en.wikipedia.org/wiki/User_agent) in case you don't know!\n","\n","There is a useful library called `user_agents`. It is a Python library that provides an easy way to identify/detect devices like mobile phones, tablets and their capabilities by parsing (browser/HTTP) user agent strings."],"id":"cooperative-matrix"},{"cell_type":"code","metadata":{"id":"maritime-channel"},"source":["# Firstly, lets install it:\n","%pip install pyyaml ua-parser user-agents"],"id":"maritime-channel","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"funded-alexandria"},"source":["Please visit it's [documentation](https://pypi.org/project/user-agents/) page.\n","\n","In this part, we will create a DataFrame which has these columns **per each session** (we assume that each rows with same IPs and user agents are within a same session). You have to group the `logs_df` DataFrame by their `client` and `user_agent` columns.\n","\n","This DataFrame columns are listed below:\n","\n"," - Browser family: *string*\n"," - OS family: *is string*\n"," - Is_bot: *boolean* \n"," - Is_pc: *boolean* \n"],"id":"funded-alexandria"},{"cell_type":"code","metadata":{"id":"widespread-package"},"source":["from user_agents import parse \n","\n","def user_agent_details(logs_df):\n","    # TODO: Implement this function that takes a pd.DataFrame\n","    # and return a # of sessions x 4 DataFrame. \n","    \n","    # Note: you must bind the user_agent functions to your DataFrame rows, hence you cannot use loops.\n","    \n","    user_agent_details_df = pd.DataFrame()\n","    \n","    ############# Your code here ############\n","    \n","    #########################################\n","    \n","    return user_agent_details_df\n","\n","user_agent_details_df = user_agent_details(logs_df)"],"id":"widespread-package","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"worldwide-removal"},"source":["## Q6. Take a deeper look! (10 points)\n","\n","Extract all the rows of the table with:\n","\n","    - client = '66.249.66.194'\n","    - user_agent = 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'\n","    \n","Then answer the following questions:\n","\n","   1. What percentage of the requests of these rows have `size = 0`?\n","   2. Take a look at the endpoints of the requests having `size = 0`. What substring do they have in common mostly?\n","   \n","Then fill the function below to plot a histogram for sizes of the requests."],"id":"worldwide-removal"},{"cell_type":"code","metadata":{"id":"pediatric-elimination"},"source":["client = '66.249.66.194'\n","user_agent = 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'\n","\n","def plot_histogram(sample_df, n_bins=10):\n","    # TODO: Implement this function that takes a pd.DataFrame\n","    # and plot a histogram with n_bins bins.\n","    \n","    ############# Your code here ############\n","    \n","    #########################################\n","    \n","sample_df = logs_df[(logs_df['client'] == client) & (logs_df['user_agent'] == user_agent)]  \n","plot_histogram(sample_df, n_bins=10)"],"id":"pediatric-elimination","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"imported-radar"},"source":["# 3. Feature Engineering\n","\n","We now have some insights of the dataset. In this part we are going to create some new features to help us better classifying **web crawlers/bots** and normal clients.\n","\n","In over the last decade, there have been numerous studies that have tried to classify web robots from web server access logs using Machine Learning approaches. But is it really possible to classify each requests individually? Nowdays, attackers and web crawler developers, implement bots which act broadly similar to humans! Therefore, it is not practical to classify the requests into crawlers and normal clients just based on one request. Here sessions just come in!\n","\n","**Session definition**: A session is a group of user interactions(requests) with your website that take place within a given time frame. For simplicity, we assume that each session only corresponds to one client(IP) and we also ignore the time frame. \n","\n","Hence you can create the session DataFrame with grouping the logs DataFrame by `client` and `user_agent` columns."],"id":"imported-radar"},{"cell_type":"markdown","metadata":{"id":"faced-willow"},"source":["## Q7. Write 10 features of web crawlers' access logs within a session which deviate them from normal client ones? Explain the intuition behind each one briefly? (5 points)\n","\n","You are free to search on the internet and read previous studies in this area but **you must mention your sources in your report**. \n","\n","This paper can be helpful: *Detecting Web Crawlers from Web Server Access Logs with Data Mining*. [[PDF]](https://www.eecs.yorku.ca/course_archive/2019-20/F/6412/project/samples/Dusan-report.pdf)"],"id":"faced-willow"},{"cell_type":"markdown","metadata":{"id":"alive-zimbabwe"},"source":["## Q8. Create the session DataFrame (10 points)\n","\n","Fill the function below to create the session DataFrame based on grouping the `logs_df` by `client` and `user_agent` columns. This DataFrame must contain one column which indicates the number of requests for each session named `requests_count`. Also **sort** the final DataFrame based on `requests_count` in **descending** order.\n","\n","\n","What can we imply from the user agents of the sessions with most request frequency? Are they normal clients? Write your thoughts in the report file."],"id":"alive-zimbabwe"},{"cell_type":"code","metadata":{"id":"reliable-shopping"},"source":["# change the data type of the client, user_agent, referer columns.\n","# It may take 1-2 minutes to execute.\n","\n","logs_df['client'] = logs_df['client'].astype(str)\n","logs_df['user_agent'] = logs_df['user_agent'].astype(str)"],"id":"reliable-shopping","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"injured-saver"},"source":["def create_session_df(logs_df):\n","    # TODO: Implement this function that takes a pd.DataFrame\n","    # and return the session DataFrame\n","    \n","    session_df = pd.DataFrame()\n","    \n","    ############# Your code here ############\n","    session_df = pd.DataFrame(logs_df.groupby(['client', 'user_agent']).size(), columns=['requests_count'])\n","    #########################################\n","    \n","    return session_df.sort_values('requests_count', ascending=False)\n","\n","session_df = create_session_df(logs_df)\n","assert session_df.shape == (295087, 1)"],"id":"injured-saver","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"satellite-murder"},"source":["## Q9. Design other useful features per session (20 points)\n","\n","In this part you will add more feature to your session DataFrame. Your final session DataFrame must include these features:\n","\n"," 1. **requests_count**: Number of HTTP request per session (You implemented this one in the previous question.\n",")\n"," 2. **STD of endpoint lengths**: Each endpoint has a length if we split the parts of it by \"/\" delimiter. Computer the STD of endpoint lenght per session.\n"," 3. **Percentage of 4xx status codes**: Its clear!\n"," 4. **Percentage of HTTP HEAD requests**: HEAD is one of the HTTP methods similar to GET, POST, PUT and Delete.\n"," 5. **Average of the `size` column**: What insights does it give us? What do YOU think?\n"," 6. **Robots.txt requests**: Indicates a web crawler attempt to access web pages.\n"," 7. **Average of time between requests per session**: Set 0 for session containing only 1 request.\n"," 8. **Percentage of requests with unassigned referrers**: Percentage of blank or unassigned referrer fields set by a user in a single session."],"id":"satellite-murder"},{"cell_type":"code","metadata":{"id":"tested-disney"},"source":["def generate_features(session_df, logs_df):\n","    # TODO: Implement this function that takes two pd.DataFrame\n","    # and return the session DataFrame with 6 features\n","    \n","    \n","    ############# Your code here ############\n","\n","    #########################################\n","    \n","    return session_df\n","\n","session_df = generate_features(session_df, logs_df)\n","assert session_df.shape == (128537, 8)"],"id":"tested-disney","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nasty-broad"},"source":["Sort the final session DataFrame in both ascending/descending based on each column. What can you imply from the results? Examine the top/least sessions' requests and its user agent, Can we conclude that are suspicious enough as crawlers? \n","\n","**Help**: After sorting the DataFrame, check out the user agents of the sessions, they may include *bot* in them. Also you can observe the requests which suspicious have sent within its session. Check the time stamps of the requests? Have been the requests sent immediately after one each?\n","\n","**Important Note**: To answer this question, you don't have to show all your DataFrames and results after sorting them here. Just play around the session DataFrame you have created and write your implications from what you see.\n","\n","But before analyzing your results, run the cell below to drop all the session with less than 10 requests."],"id":"nasty-broad"},{"cell_type":"code","metadata":{"id":"sound-rating"},"source":["# We drop all the session with less than 10 requests.\n","session_df.drop(session_df[session_df[\"requests_count\"] < 10].index, inplace=True)\n","session_df"],"id":"sound-rating","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"spanish-truck"},"source":["## Q10. (Optional) The last but not the least! (+5 points)\n","\n","There is another novel feature which you must implement it in this section. Read about it below:\n","\n","\n","**Percentage of consecutive repeated HTTP requests** – a numerical attribute calculated as **the number of repeated requests sent in sequence belonging to the same web directory sent by a user during a session**. For instance, a series of requests for web pages matching pattern ‘/cshome/course/\\*.\\* will be marked as consecutive repeated HTTP requests. However, a request to web page ‘/cshome/index.html’ followed by a request to a web page ‘cshome/courses/index.html’ will **not** be marked as consecutive repeated requests.\n","\n","\n","Fill the function below to add this column to the existing  session DataFrame."],"id":"spanish-truck"},{"cell_type":"code","metadata":{"id":"remarkable-classification"},"source":["def generate_features(session_df, logs_df):\n","    # TODO: Implement this function that takes two pd.DataFrame\n","    # and return the session DataFrame with 7 features\n","\n","    \n","    ############# Your code here ############\n","\n","    #########################################\n","    \n","    return session_df\n","\n","session_df = generate_features(session_df, logs_df)\n","assert session_df.shape == (128537, 9)"],"id":"remarkable-classification","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"brutal-treatment"},"source":["# 4. Submission\n","\n","Please read the notes here carefully:\n","\n","1. Your codes **must not** include any loops. You just have to use Pandas functionality to answer the questions. Answers which have loops will lose points.\n","\n","2. The more beautiful and insightfull your plots and diagrams are, the more points you get. So please take your time and concentration to prepare a good report with nice diagrams. The main goal of this assignment is to challenge your abilities in python packages not in theories.\n","\n","3. Copy and paste all the functions you have implemented in a `Utils.py` file. Note that you won't need to add any other functions and you just have to copy and paste the functions which have ***\\\"Your code here\\\"*** comments in its body.\n","\n","4. The file you upload must be named as `[Student ID]-[Your name].zip` and it must contain **only 3 files**:\n","\n","  - `LogServerAnalysis.ipynb`\n","  - `Utils.py`\n","  - `Report.pdf`\n","  \n","In case you have any questions, contact **mohammad99hashemi@gmail.com**"],"id":"brutal-treatment"}]}